{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Coupled Multi-Layer Attentions for Co-Extraction of Aspect and Opinion Terms](https://arxiv.org/pdf/1702.01776.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The aim of this paper is to extract aspect and opinion words for a review text.<br>\n",
    "*“This little place has a cute interior decor and affordable prices”*<br>\n",
    "Aspect terms: interior decor, prices<br>\n",
    "Opinion terms: cute, affordable<br>\n",
    "\n",
    "The proposed model uses a coupled attention over GRUs to learn aspect and opinion terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "* Two attention models one for aspect and one for opinion, each attention aims to learn:\n",
    "\t- a prototype vector for aspect or opinion\n",
    "\t- a high-level feature vector for each token\n",
    "\t- an attention score for each token in the sentence.\n",
    "\n",
    "* To exploit the relations between aspect and opinion terms the attentions are coupled such that their learning effects one another\n",
    "\n",
    "* To capture indirect relations they have proposed multiple layers of attention\n",
    "\n",
    "* A a tensor operation is performed to capture different syntactic representations of the same input. This is similar to the different kinds of parse trees that can be formed for a sentence.\n",
    "\n",
    "* To solve the problem of multiple word or phrase as aspect words they have considered a BIO encoding scheme where a word can wither be Beginning of the aspect, In the aspect or Out of the aspect phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "![Coupled Attention Model](../resources/coupled-attention.png)\n",
    "\n",
    "1. Use GRU to find hidden state representation $h_i$ of every word $w_i$ in a sentence\n",
    "2. Randomly initalize prototype vector $u^m ~ U[-0.2,0.2]$, where m can be aspect or opinion term\n",
    "3. Calculate $\\beta_i^m = f^a(h_i, u^m, u^{\\bar{m}}) = \\tanh(h_i^T G^m u^m ; h_i^T D^m u^{\\bar{m}})$\n",
    "4. $r_i^m$ = GRU($\\beta_i^m$) - attention vector - high level representation of each input token\n",
    "5. $e_i^m$ = $v^{m^{T}}$ $r_i^m$ - attention score\n",
    "\n",
    "[TODO: add the multi-layer part]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "* This seems like a good model since it is able to capture semantic and syntactic nature of a text.\n",
    "* The results are shown to perform across different domains of reviews. This proves that the model actually learns the syntactic placement of aspect and opinion words in a sentence.\n",
    "* Need to verify whether the model needs to be so complex.\n",
    "* Memory networks can be explored to check if they perform better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "* The dataset found for this task only has aspect terms and not opinion terms. Thus only a single attention model is used in  the implementation.\n",
    "* The word2vec could not be trained for yelp restaurant review dataset as it is huge and yelp dataset set contains mixed reviews and not restaurant reviews seperately\n",
    "* The model is overfitting due to class imbalance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
